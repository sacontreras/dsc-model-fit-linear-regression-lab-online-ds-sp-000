{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Model Fit in Linear Regression - Lab\n","\n","## Introduction\n","In this lab, you'll learn how to evaluate your model results, and you'll learn methods to select the appropriate features using stepwise selection.\n","\n","## Objectives\n","You will be able to:\n","* Analyze the results of regression and R-squared and adjusted-R-squared \n","* Understand and apply forward and backward predictor selection\n","\n","## The Boston Housing Data once more\n","\n","We pre-processed the Boston Housing Data the same way we did before:\n","\n","- We dropped \"ZN\" and \"NOX\" completely\n","- We categorized \"RAD\" in 3 bins and \"TAX\" in 4 bins\n","- We transformed \"RAD\" and \"TAX\" to dummy variables and dropped the first variable\n","- We used min-max-scaling on \"B\", \"CRIM\" and \"DIS\" (and logtransformed all of them first, except \"B\")\n","- We used standardization on \"AGE\", \"INDUS\", \"LSTAT\" and \"PTRATIO\" (and logtransformed all of them first, except for \"AGE\") "]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.datasets import load_boston\n","boston = load_boston()\n","\n","boston_features = pd.DataFrame(boston.data, columns = boston.feature_names)\n","boston_features = boston_features.drop([\"NOX\",\"ZN\"],axis=1)\n","\n","# first, create bins for based on the values observed. 3 values will result in 2 bins\n","bins = [0,6,  24]\n","bins_rad = pd.cut(boston_features['RAD'], bins)\n","bins_rad = bins_rad.cat.as_unordered()\n","\n","# first, create bins for based on the values observed. 4 values will result in 3 bins\n","bins = [0, 270, 360, 712]\n","bins_tax = pd.cut(boston_features['TAX'], bins)\n","bins_tax = bins_tax.cat.as_unordered()\n","\n","tax_dummy = pd.get_dummies(bins_tax, prefix=\"TAX\", drop_first=True)\n","rad_dummy = pd.get_dummies(bins_rad, prefix=\"RAD\", drop_first=True)\n","boston_features = boston_features.drop([\"RAD\",\"TAX\"], axis=1)\n","boston_features = pd.concat([boston_features, rad_dummy, tax_dummy], axis=1)\n","\n","age = boston_features[\"AGE\"]\n","b = boston_features[\"B\"]\n","logcrim = np.log(boston_features[\"CRIM\"])\n","logdis = np.log(boston_features[\"DIS\"])\n","logindus = np.log(boston_features[\"INDUS\"])\n","loglstat = np.log(boston_features[\"LSTAT\"])\n","logptratio = np.log(boston_features[\"PTRATIO\"])\n","\n","# minmax scaling\n","boston_features[\"B\"] = (b-min(b))/(max(b)-min(b))\n","boston_features[\"CRIM\"] = (logcrim-min(logcrim))/(max(logcrim)-min(logcrim))\n","boston_features[\"DIS\"] = (logdis-min(logdis))/(max(logdis)-min(logdis))\n","\n","#standardization\n","boston_features[\"AGE\"] = (age-np.mean(age))/np.sqrt(np.var(age))\n","boston_features[\"INDUS\"] = (logindus-np.mean(logindus))/np.sqrt(np.var(logindus))\n","boston_features[\"LSTAT\"] = (loglstat-np.mean(loglstat))/np.sqrt(np.var(loglstat))\n","boston_features[\"PTRATIO\"] = (logptratio-np.mean(logptratio))/(np.sqrt(np.var(logptratio)))"]},{"cell_type":"markdown","metadata":{},"source":["## Perform stepwise selection"]},{"cell_type":"markdown","metadata":{},"source":["The code for stepwise selection is copied below. Use this code provided on your preprocessed Boston Housing Data."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import statsmodels.api as sm\n","\n","def stepwise_selection(X, y, \n","                       initial_list=[], \n","                       threshold_in=0.01, \n","                       threshold_out = 0.05, \n","                       verbose=True):\n","    \"\"\" Perform a forward-backward feature selection \n","    based on p-value from statsmodels.api.OLS\n","    Arguments:\n","        X - pandas.DataFrame with candidate features\n","        y - list-like with the target\n","        initial_list - list of features to start with (column names of X)\n","        threshold_in - include a feature if its p-value < threshold_in\n","        threshold_out - exclude a feature if its p-value > threshold_out\n","        verbose - whether to print the sequence of inclusions and exclusions\n","    Returns: list of selected features \n","    Always set threshold_in < threshold_out to avoid infinite looping.\n","    See https://en.wikipedia.org/wiki/Stepwise_regression for the details\n","    \"\"\"\n","    included = list(initial_list)\n","    while True:\n","        changed=False\n","        # forward step\n","        excluded = list(set(X.columns)-set(included))\n","        new_pval = pd.Series(index=excluded)\n","        for new_column in excluded:\n","            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n","            new_pval[new_column] = model.pvalues[new_column]\n","        best_pval = new_pval.min()\n","        if best_pval < threshold_in:\n","            best_feature = new_pval.idxmin()\n","            included.append(best_feature)\n","            changed=True\n","            if verbose:\n","                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n","\n","        # backward step\n","        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n","        # use all coefs except intercept\n","        pvalues = model.pvalues.iloc[1:]\n","        worst_pval = pvalues.max() # null if pvalues is empty\n","        if worst_pval > threshold_out:\n","            changed=True\n","            worst_feature = pvalues.argmax()\n","            included.remove(worst_feature)\n","            if verbose:\n","                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n","        if not changed:\n","            break\n","    return included"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MEDV</th>\n      <th>CRIM</th>\n      <th>INDUS</th>\n      <th>CHAS</th>\n      <th>RM</th>\n      <th>AGE</th>\n      <th>DIS</th>\n      <th>PTRATIO</th>\n      <th>B</th>\n      <th>LSTAT</th>\n      <th>RAD_(6, 24]</th>\n      <th>TAX_(270, 360]</th>\n      <th>TAX_(360, 712]</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>24.0</td>\n      <td>0.000000</td>\n      <td>-1.704344</td>\n      <td>0.0</td>\n      <td>6.575</td>\n      <td>-0.120013</td>\n      <td>0.542096</td>\n      <td>-1.443977</td>\n      <td>1.000000</td>\n      <td>-1.275260</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>1</td>\n      <td>21.6</td>\n      <td>0.153211</td>\n      <td>-0.263239</td>\n      <td>0.0</td>\n      <td>6.421</td>\n      <td>0.367166</td>\n      <td>0.623954</td>\n      <td>-0.230278</td>\n      <td>1.000000</td>\n      <td>-0.263711</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>34.7</td>\n      <td>0.153134</td>\n      <td>-0.263239</td>\n      <td>0.0</td>\n      <td>7.185</td>\n      <td>-0.265812</td>\n      <td>0.623954</td>\n      <td>-0.230278</td>\n      <td>0.989737</td>\n      <td>-1.627858</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>33.4</td>\n      <td>0.171005</td>\n      <td>-1.778965</td>\n      <td>0.0</td>\n      <td>6.998</td>\n      <td>-0.809889</td>\n      <td>0.707895</td>\n      <td>0.165279</td>\n      <td>0.994276</td>\n      <td>-2.153192</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>36.2</td>\n      <td>0.250315</td>\n      <td>-1.778965</td>\n      <td>0.0</td>\n      <td>7.147</td>\n      <td>-0.511180</td>\n      <td>0.707895</td>\n      <td>0.165279</td>\n      <td>1.000000</td>\n      <td>-1.162114</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <td>501</td>\n      <td>22.4</td>\n      <td>0.240099</td>\n      <td>0.410792</td>\n      <td>0.0</td>\n      <td>6.593</td>\n      <td>0.018673</td>\n      <td>0.331081</td>\n      <td>1.095518</td>\n      <td>0.987619</td>\n      <td>-0.169811</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>502</td>\n      <td>20.6</td>\n      <td>0.206118</td>\n      <td>0.410792</td>\n      <td>0.0</td>\n      <td>6.120</td>\n      <td>0.288933</td>\n      <td>0.297277</td>\n      <td>1.095518</td>\n      <td>1.000000</td>\n      <td>-0.274682</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>503</td>\n      <td>23.9</td>\n      <td>0.236926</td>\n      <td>0.410792</td>\n      <td>0.0</td>\n      <td>6.976</td>\n      <td>0.797449</td>\n      <td>0.274575</td>\n      <td>1.095518</td>\n      <td>1.000000</td>\n      <td>-1.067939</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>504</td>\n      <td>22.0</td>\n      <td>0.298671</td>\n      <td>0.410792</td>\n      <td>0.0</td>\n      <td>6.794</td>\n      <td>0.736996</td>\n      <td>0.315551</td>\n      <td>1.095518</td>\n      <td>0.991301</td>\n      <td>-0.836660</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <td>505</td>\n      <td>11.9</td>\n      <td>0.210954</td>\n      <td>0.410792</td>\n      <td>0.0</td>\n      <td>6.030</td>\n      <td>0.434732</td>\n      <td>0.335545</td>\n      <td>1.095518</td>\n      <td>1.000000</td>\n      <td>-0.510809</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>506 rows × 13 columns</p>\n</div>","text/plain":"     MEDV      CRIM     INDUS  CHAS     RM       AGE       DIS   PTRATIO  \\\n0    24.0  0.000000 -1.704344   0.0  6.575 -0.120013  0.542096 -1.443977   \n1    21.6  0.153211 -0.263239   0.0  6.421  0.367166  0.623954 -0.230278   \n2    34.7  0.153134 -0.263239   0.0  7.185 -0.265812  0.623954 -0.230278   \n3    33.4  0.171005 -1.778965   0.0  6.998 -0.809889  0.707895  0.165279   \n4    36.2  0.250315 -1.778965   0.0  7.147 -0.511180  0.707895  0.165279   \n..    ...       ...       ...   ...    ...       ...       ...       ...   \n501  22.4  0.240099  0.410792   0.0  6.593  0.018673  0.331081  1.095518   \n502  20.6  0.206118  0.410792   0.0  6.120  0.288933  0.297277  1.095518   \n503  23.9  0.236926  0.410792   0.0  6.976  0.797449  0.274575  1.095518   \n504  22.0  0.298671  0.410792   0.0  6.794  0.736996  0.315551  1.095518   \n505  11.9  0.210954  0.410792   0.0  6.030  0.434732  0.335545  1.095518   \n\n            B     LSTAT  RAD_(6, 24]  TAX_(270, 360]  TAX_(360, 712]  \n0    1.000000 -1.275260            0               1               0  \n1    1.000000 -0.263711            0               0               0  \n2    0.989737 -1.627858            0               0               0  \n3    0.994276 -2.153192            0               0               0  \n4    1.000000 -1.162114            0               0               0  \n..        ...       ...          ...             ...             ...  \n501  0.987619 -0.169811            0               1               0  \n502  1.000000 -0.274682            0               1               0  \n503  1.000000 -1.067939            0               1               0  \n504  0.991301 -0.836660            0               1               0  \n505  1.000000 -0.510809            0               1               0  \n\n[506 rows x 13 columns]"},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# Your code here\n","target = 'MEDV'\n","data_fin = pd.concat([pd.DataFrame(data=boston.target, columns=[target]), boston_features], axis=1, join='inner')\n","data_fin"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":"/Users/stevencontreras/anaconda3/envs/learn-env/lib/python3.6/site-packages/numpy/core/fromnumeric.py:2495: FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n  return ptp(axis=axis, out=out, **kwargs)\nAdd  LSTAT                          with p-value 9.27989e-122\nAdd  RM                             with p-value 1.98621e-16\nAdd  PTRATIO                        with p-value 2.5977e-12\nAdd  DIS                            with p-value 2.85496e-09\nAdd  B                              with p-value 2.77572e-06\nAdd  INDUS                          with p-value 0.0017767\nAdd  CHAS                           with p-value 0.0004737\n\nresulting features:\n['LSTAT', 'RM', 'PTRATIO', 'DIS', 'B', 'INDUS', 'CHAS']\n"}],"source":["sig_features = stepwise_selection(boston_features, data_fin[target], verbose = True)\n","print(\"\\nresulting features:\\n{}\".format(sig_features))"]},{"cell_type":"markdown","metadata":{},"source":["### Build the final model again in Statsmodels"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.773</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.770</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   242.7</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 29 Oct 2019</td> <th>  Prob (F-statistic):</th> <td>4.89e-156</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>18:00:42</td>     <th>  Log-Likelihood:    </th> <td> -1464.7</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   2945.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   498</td>      <th>  BIC:               </th> <td>   2979.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     7</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    5.0123</td> <td>    2.829</td> <td>    1.772</td> <td> 0.077</td> <td>   -0.545</td> <td>   10.570</td>\n</tr>\n<tr>\n  <th>LSTAT</th>     <td>   -5.6444</td> <td>    0.320</td> <td>  -17.629</td> <td> 0.000</td> <td>   -6.274</td> <td>   -5.015</td>\n</tr>\n<tr>\n  <th>RM</th>        <td>    2.8712</td> <td>    0.388</td> <td>    7.405</td> <td> 0.000</td> <td>    2.109</td> <td>    3.633</td>\n</tr>\n<tr>\n  <th>PTRATIO</th>   <td>   -1.3564</td> <td>    0.227</td> <td>   -5.983</td> <td> 0.000</td> <td>   -1.802</td> <td>   -0.911</td>\n</tr>\n<tr>\n  <th>DIS</th>       <td>   -9.7229</td> <td>    1.326</td> <td>   -7.333</td> <td> 0.000</td> <td>  -12.328</td> <td>   -7.118</td>\n</tr>\n<tr>\n  <th>B</th>         <td>    4.0619</td> <td>    0.934</td> <td>    4.347</td> <td> 0.000</td> <td>    2.226</td> <td>    5.898</td>\n</tr>\n<tr>\n  <th>INDUS</th>     <td>   -1.2099</td> <td>    0.334</td> <td>   -3.619</td> <td> 0.000</td> <td>   -1.867</td> <td>   -0.553</td>\n</tr>\n<tr>\n  <th>CHAS</th>      <td>    2.7988</td> <td>    0.795</td> <td>    3.519</td> <td> 0.000</td> <td>    1.236</td> <td>    4.362</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>105.185</td> <th>  Durbin-Watson:     </th> <td>   1.099</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 423.621</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.878</td>  <th>  Prob(JB):          </th> <td>1.03e-92</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 7.124</td>  <th>  Cond. No.          </th> <td>    96.7</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.773\nModel:                            OLS   Adj. R-squared:                  0.770\nMethod:                 Least Squares   F-statistic:                     242.7\nDate:                Tue, 29 Oct 2019   Prob (F-statistic):          4.89e-156\nTime:                        18:00:42   Log-Likelihood:                -1464.7\nNo. Observations:                 506   AIC:                             2945.\nDf Residuals:                     498   BIC:                             2979.\nDf Model:                           7                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      5.0123      2.829      1.772      0.077      -0.545      10.570\nLSTAT         -5.6444      0.320    -17.629      0.000      -6.274      -5.015\nRM             2.8712      0.388      7.405      0.000       2.109       3.633\nPTRATIO       -1.3564      0.227     -5.983      0.000      -1.802      -0.911\nDIS           -9.7229      1.326     -7.333      0.000     -12.328      -7.118\nB              4.0619      0.934      4.347      0.000       2.226       5.898\nINDUS         -1.2099      0.334     -3.619      0.000      -1.867      -0.553\nCHAS           2.7988      0.795      3.519      0.000       1.236       4.362\n==============================================================================\nOmnibus:                      105.185   Durbin-Watson:                   1.099\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              423.621\nSkew:                           0.878   Prob(JB):                     1.03e-92\nKurtosis:                       7.124   Cond. No.                         96.7\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# Your code here\n","from statsmodels.formula.api import ols\n","\n","f = target + '~' + \"+\".join(sig_features)\n","model_fit_results = ols(formula=f, data=data_fin).fit()\n","model_fit_results.summary()"]},{"cell_type":"markdown","metadata":{},"source":["The stepwise procedure mentions that \"INDUS\" was added with a p-value of 0.0017767, but our statsmodels output returns a p-value of 0.000. Use some of the stepwise procedure logic to find the intuition behind this!"]},{"cell_type":"markdown","metadata":{},"source":["## Use Feature ranking with recursive feature elimination"]},{"cell_type":"markdown","metadata":{},"source":["Use feature ranking to select the 5 most important features"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"[False False  True  True False  True False  True  True False False False]\n[4 7 1 1 8 1 3 1 1 5 6 2]\n[ 2.93498961  3.43718997 -6.58036332  4.65357304 -6.25217488]\n-0.49739821537886897\n"}],"source":["# Your code here\n","from sklearn.feature_selection import RFE\n","from sklearn.linear_model import LinearRegression\n","\n","linreg = LinearRegression()\n","selector = RFE(linreg, n_features_to_select = 5)\n","selector = selector.fit(boston_features, data_fin[target])\n","\n","print(selector.support_)\n","print(selector.ranking_)\n","estimators = selector.estimator_\n","print(estimators.coef_)\n","print(estimators.intercept_)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"data":{"text/plain":"['CHAS', 'RM', 'DIS', 'B', 'LSTAT']"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["sig_features = list(filter(lambda e: selector.support_[e[0]]==True, enumerate(boston_features.columns)))\n","sig_features = list(map(lambda t: t[1], sig_features))\n","sig_features"]},{"cell_type":"markdown","metadata":{},"source":["Fit the linear regression model again using the 5 columns selected"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"data":{"text/html":"<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>MEDV</td>       <th>  R-squared:         </th> <td>   0.743</td> \n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.740</td> \n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   289.1</td> \n</tr>\n<tr>\n  <th>Date:</th>             <td>Tue, 29 Oct 2019</td> <th>  Prob (F-statistic):</th> <td>5.96e-145</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>18:00:42</td>     <th>  Log-Likelihood:    </th> <td> -1496.5</td> \n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>   506</td>      <th>  AIC:               </th> <td>   3005.</td> \n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>   500</td>      <th>  BIC:               </th> <td>   3030.</td> \n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>   -0.4974</td> <td>    2.867</td> <td>   -0.174</td> <td> 0.862</td> <td>   -6.130</td> <td>    5.135</td>\n</tr>\n<tr>\n  <th>CHAS</th>      <td>    2.9350</td> <td>    0.834</td> <td>    3.518</td> <td> 0.000</td> <td>    1.296</td> <td>    4.574</td>\n</tr>\n<tr>\n  <th>RM</th>        <td>    3.4372</td> <td>    0.405</td> <td>    8.497</td> <td> 0.000</td> <td>    2.642</td> <td>    4.232</td>\n</tr>\n<tr>\n  <th>DIS</th>       <td>   -6.5804</td> <td>    1.116</td> <td>   -5.894</td> <td> 0.000</td> <td>   -8.774</td> <td>   -4.387</td>\n</tr>\n<tr>\n  <th>B</th>         <td>    4.6536</td> <td>    0.989</td> <td>    4.707</td> <td> 0.000</td> <td>    2.711</td> <td>    6.596</td>\n</tr>\n<tr>\n  <th>LSTAT</th>     <td>   -6.2522</td> <td>    0.330</td> <td>  -18.951</td> <td> 0.000</td> <td>   -6.900</td> <td>   -5.604</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>82.869</td> <th>  Durbin-Watson:     </th> <td>   1.032</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.000</td> <th>  Jarque-Bera (JB):  </th> <td> 269.427</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.743</td> <th>  Prob(JB):          </th> <td>3.12e-59</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 6.251</td> <th>  Cond. No.          </th> <td>    91.5</td>\n</tr>\n</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.","text/plain":"<class 'statsmodels.iolib.summary.Summary'>\n\"\"\"\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                   MEDV   R-squared:                       0.743\nModel:                            OLS   Adj. R-squared:                  0.740\nMethod:                 Least Squares   F-statistic:                     289.1\nDate:                Tue, 29 Oct 2019   Prob (F-statistic):          5.96e-145\nTime:                        18:00:42   Log-Likelihood:                -1496.5\nNo. Observations:                 506   AIC:                             3005.\nDf Residuals:                     500   BIC:                             3030.\nDf Model:                           5                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P>|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -0.4974      2.867     -0.174      0.862      -6.130       5.135\nCHAS           2.9350      0.834      3.518      0.000       1.296       4.574\nRM             3.4372      0.405      8.497      0.000       2.642       4.232\nDIS           -6.5804      1.116     -5.894      0.000      -8.774      -4.387\nB              4.6536      0.989      4.707      0.000       2.711       6.596\nLSTAT         -6.2522      0.330    -18.951      0.000      -6.900      -5.604\n==============================================================================\nOmnibus:                       82.869   Durbin-Watson:                   1.032\nProb(Omnibus):                  0.000   Jarque-Bera (JB):              269.427\nSkew:                           0.743   Prob(JB):                     3.12e-59\nKurtosis:                       6.251   Cond. No.                         91.5\n==============================================================================\n\nWarnings:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\"\"\""},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# Your code here\n","f = target + '~' + \"+\".join(sig_features)\n","model_fit_results = ols(formula=f, data=data_fin).fit()\n","model_fit_results.summary()"]},{"cell_type":"markdown","metadata":{},"source":["Now, predict $\\hat y$ using your model. you can use `.predict()` in scikit-learn"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":"array([31.16165902, 23.76929453, 34.8764376 , 36.98692436, 31.32930715,\n       29.06938762, 18.83320099, 14.70915944,  8.01751688, 14.90750061,\n       14.49961186, 17.84334784, 15.96671008, 23.35014491, 21.5475942 ,\n       22.80420786, 25.67823103, 17.66801955, 17.36837517, 19.86812537,\n       12.67589909, 18.42874828, 15.95977353, 14.09380242, 16.3420057 ,\n       13.99038842, 16.58165948, 15.0909717 , 20.71269856, 22.06463506,\n       11.91943166, 19.11352766,  9.2779531 , 14.33996426, 13.34927881,\n       22.57757018, 20.30738333, 22.88337432, 21.8018023 , 31.92538034,\n       41.52527107, 31.06370984, 27.07153202, 24.77046631, 21.62254369,\n       20.00275986, 16.96128095, 14.4887785 ,  7.13719738, 14.42912675,\n       17.2790188 , 21.48105026, 28.91702238, 22.28080957, 15.84257515,\n       31.73722975, 26.72444484, 32.38609452, 24.47989229, 21.05648509,\n       16.58337781, 16.34295586, 26.03847483, 23.20882414, 25.68084118,\n       29.5199626 , 19.61369356, 22.40808622, 16.44091927, 21.58879946,\n       26.63121477, 20.96993535, 27.55433937, 24.76390537, 26.77946793,\n       23.64749623, 20.76349313, 22.01106198, 19.8438734 , 22.19717234,\n       30.2844787 , 26.65887163, 26.33164552, 24.6365546 , 23.23301352,\n       28.24826213, 19.12576907, 24.32850101, 32.10672912, 31.97818138,\n       25.39705173, 26.11938057, 25.84269183, 27.90838823, 22.52538516,\n       28.30059511, 21.55106014, 38.4819158 , 39.30393543, 32.2226848 ,\n       26.0929423 , 28.35160794, 19.98982205, 20.42376071, 21.72121536,\n       18.02104751, 16.55455204, 20.50030464, 22.84356682, 19.36202497,\n       20.89459242, 25.37576518, 18.10369451, 17.97097542, 23.89199212,\n       17.73498044, 21.67407291, 22.7636978 , 17.71362303, 18.81742072,\n       19.25780492, 19.73146433, 17.35369613, 13.46616205, 17.40149903,\n       19.63980912, 12.05031727, 17.46411241, 21.17171641, 16.371211  ,\n       22.89344411, 22.55375075, 23.5296126 , 18.37067806, 15.341256  ,\n       19.38158562, 18.07062758, 21.71718251, 15.97650024, 18.54408536,\n       15.81302621,  8.57706876, 15.66409923, 12.92599064, 10.04244695,\n       12.02847158, 15.41513393,  9.74849021, 10.54290749, 14.79613397,\n       21.05331449, 18.88894128, 21.41602014, 17.14510251, 22.4608601 ,\n       19.88101486, 13.19402919, 35.0420452 , 28.58793469, 28.77348461,\n       33.5427297 , 47.07558868, 50.08276226, 46.17484179, 21.27241216,\n       22.24800422, 40.50651203, 18.9434751 , 22.61075603, 22.87164458,\n       17.8958007 , 20.50564361, 17.70787384, 25.59596446, 22.92787057,\n       31.02448185, 22.22479009, 28.40083253, 29.58741951, 33.45095255,\n       31.99805957, 24.2666834 , 34.61346518, 30.75498196, 17.87490032,\n       20.09540457, 37.28165815, 28.99102504, 31.54337225, 32.12870148,\n       30.70481578, 30.99016552, 37.62332953, 30.62747755, 31.19933299,\n       40.08691761, 34.07547034, 25.17870951, 28.9347293 , 31.71698662,\n       32.3734075 , 24.35386493, 38.42037838, 37.67359888, 41.18412161,\n       20.78060271, 21.88280477, 14.76026606, 20.81294894, 14.03867953,\n       19.13650972, 13.9859968 , 19.52490218, 23.84954058,  8.41900315,\n       23.17846924, 22.05043467, 24.90572456, 19.57888955, 26.13847983,\n       29.36365933, 18.26571797, 28.49592139, 27.52534865, 39.71752951,\n       40.0943624 , 41.5889475 , 31.01071803, 37.77822159, 34.34768271,\n       20.34999012, 33.54434912, 44.56825799, 39.4254508 , 29.49733674,\n       21.41914612, 27.38044028, 34.27022286, 26.93699705, 25.88284535,\n       22.38125573, 18.77014894, 20.16871526, 28.34170923, 16.07055588,\n       12.24643414, 21.2513287 , 20.42328896, 21.86535198, 26.9468931 ,\n       27.44369848, 32.22409965, 33.7743568 , 38.34696259, 24.36999452,\n       20.03632037, 37.74902468, 40.38012207, 31.08096367, 30.5933566 ,\n       28.27933401, 32.1797757 , 37.13639506, 27.09413255, 30.24051941,\n       21.90284558, 22.98794473, 34.20746288, 39.87804476, 21.3978359 ,\n       18.37922815, 26.87191518, 26.55187051, 34.7775543 , 38.31825254,\n       37.35122311, 33.92195982, 36.31291953, 26.98303824, 32.24452212,\n       37.87291111, 32.61760806, 42.11363651, 42.44242283, 26.55069973,\n       23.87573499, 17.17576169, 24.53859919, 24.25266772, 22.48146968,\n       35.71542497, 36.00629022, 31.33248197, 23.13337182, 20.72420494,\n       28.0725372 , 25.86987478, 15.20555312, 28.25607299, 31.18104672,\n       28.24707546, 23.65598077, 24.13917641, 31.92170953, 30.00086745,\n       25.71557376, 32.10902865, 28.49251403, 32.91167529, 22.61773325,\n       16.75248398, 28.91208583, 21.36039335, 25.87894476, 24.98927368,\n       19.54914206, 15.30752933, 16.35878828, 23.25539875, 19.85125748,\n       26.52033527, 26.83712164, 24.38955501, 18.78435465, 28.06698464,\n       29.64337989, 27.28138192, 18.86818638, 20.6587547 , 25.36175718,\n       22.07715222, 17.27069911, 22.8283453 , 27.55071352, 25.72889205,\n       23.30692505, 21.00223224, 20.09281805, 23.34778067, 21.69631256,\n       22.13054274, 30.9052643 , 24.02633784, 26.83119421, 31.61912595,\n       19.43773399, 16.86535894, 26.31933137, 27.39796835, 28.46040036,\n       26.84188762, 27.18457358, 21.40421425, 29.58400111, 20.22326838,\n       24.91795123, 21.29416859, 24.56405545, 24.9692114 , 21.12688409,\n       26.97590281, 20.20085706, 21.18966642, 21.81627148, 42.66847552,\n       19.16283381, 16.25652033, 11.23159491, 32.91621857, 40.28426573,\n       44.36794984, 26.27920591, 28.65877557,  8.64483718,  5.17736637,\n       26.50122757, 18.0580462 , 19.86436611, 17.24760122, 17.54638742,\n       22.53680616, 18.77627754, 14.02256228, 13.6365335 ,  6.24792183,\n       10.64055401,  9.30595492,  9.12314247,  8.75829066, 14.56339343,\n       17.4022879 , 17.08970139, 11.11012849, 20.53639725, 18.71856016,\n       20.27800706, 18.90253847, 16.37736824, 11.19935873, 12.07476207,\n       14.24536584, 18.36597991, 18.22480799, 15.01732078, 11.61971798,\n       14.95129763,  9.83414093, 21.0147197 , 12.39776946, 18.04155361,\n       19.29490008, 14.83115314,  2.71353953, 12.19777563,  2.09441548,\n       10.19396949, 12.5862556 ,  8.68503105, 12.04649428, 14.39602628,\n       20.36931542, 18.50720784, 17.95153628, 10.79853731, 12.08721809,\n        9.82522685, 14.29129057, 16.47813759, 13.15739072, 12.16064068,\n       15.44214799, 15.89171306, 19.67087661, 16.48078299, 16.51368418,\n       13.73725671, 15.00310155,  9.87323182,  7.34465629, 14.27096547,\n       15.1462937 , 18.11979315, 19.36728257, 18.883041  , 12.69288936,\n       12.24068255, 18.05634122, 19.23266541, 18.00593511, 17.15451535,\n       15.62008108, 19.25308309, 18.63077425, 22.51718419, 14.69189623,\n       14.90904094, 11.89374615, 12.66953877, 17.51927599, 19.32361191,\n       18.765025  , 20.46673024, 20.63319873, 24.40265469, 20.62863258,\n       17.56036239, 12.7228374 , 14.60024337, 16.08792383, 17.91001994,\n       18.24465013, 20.83409892, 20.57270009, 24.76095345, 14.67585782,\n       13.92662321, 18.66123891, 11.3170395 , 17.97569876, 21.78361306,\n       22.5051671 , 27.69395984, 29.7557336 , 20.62543931, 18.43227891,\n       22.37979327, 18.45332472, 20.80763544, 16.11329454, 12.52989456,\n        8.7963685 , 17.79641771, 20.69168357, 20.50875564, 19.97424638,\n       15.91177944, 13.08296589, 18.59795129, 20.78875778, 17.61546127,\n       19.63678336, 25.6430062 , 24.95294736, 33.00415532, 30.62246451,\n       25.86809189])"},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["# Your code here\n","y_pred = model_fit_results.predict()\n","y_pred"]},{"cell_type":"markdown","metadata":{},"source":["Now, using the formulas of R-squared and adjusted-R-squared below, and your Python/numpy knowledge, compute them and contrast them with the R-squared and adjusted-R-squared in your statsmodels output using stepwise selection. Which of the two models would you prefer?"]},{"cell_type":"markdown","metadata":{},"source":["$SS_{residual} = \\sum (y - \\hat{y})^2 $\n","\n","$SS_{total} = \\sum (y - \\bar{y})^2 $\n","\n","$R^2 = 1- \\dfrac{SS_{residual}}{SS_{total}}$\n","\n","$R^2_{adj}= 1-(1-R^2)\\dfrac{n-1}{n-p-1}$"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":"0.7404105820561162\n0.7404105820561162\n"}],"source":["# Your code here\n","\n","SSR = ((data_fin['MEDV'] - y_pred)**2).sum()\n","SST = ((data_fin['MEDV'] - data_fin['MEDV'].mean())**2).sum()\n","R_SQUARED = 1 - (SSR/SST)\n","R_SQUARED_ADJ = 1 - (1 - R_SQUARED)*((len(data_fin)-1)/(len(data_fin)-5-1)) # p is the number of indep vars used in the model\n","\n","# r_squared is 0.742981  \n","print(R_SQUARED_ADJ)\n","# adjusted_r_squared is 0.740411\n","print(R_SQUARED_ADJ)"]},{"cell_type":"markdown","metadata":{},"source":["## Level up - Optional"]},{"cell_type":"markdown","metadata":{},"source":["- Perform variable selection using forward selection, using this resource: https://planspace.org/20150423-forward_selection_with_statsmodels/. Note that this time features are added based on the adjusted-R-squared!\n","- Tweak the code in the `stepwise_selection()`-function written above to just perform forward selection based on the p-value."]},{"cell_type":"markdown","metadata":{},"source":["## Summary\n","Great! You now performed your own feature selection methods!"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"nbformat":4,"nbformat_minor":2}